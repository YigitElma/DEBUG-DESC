{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': '0',\n",
       "  'type': 'NVIDIA A100-SXM4-40GB',\n",
       "  'uuid': 'GPU-aae23f7e-562e-1c68-1315-4125bc336161',\n",
       "  'mem_used': 1,\n",
       "  'mem_total': 40960,\n",
       "  'mem_used_percent': 0.00244140625},\n",
       " {'index': '1',\n",
       "  'type': 'NVIDIA A100-SXM4-40GB',\n",
       "  'uuid': 'GPU-9b73e321-cbb3-d292-70af-d2fd4b5fed1f',\n",
       "  'mem_used': 1,\n",
       "  'mem_total': 40960,\n",
       "  'mem_used_percent': 0.00244140625}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nvgpu\n",
    "\n",
    "nvgpu.gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "\n",
    "# we want to JIT the methods of a class on specific devices\n",
    "# for convenience, we define a decorator that does this for us, this will use\n",
    "# the device_id attribute of the class to determine the device to JIT on\n",
    "def jit_with_device(method):\n",
    "    \"\"\"Decorator to Just-in-time compile a class method with a dynamic device.\n",
    "\n",
    "    Decorates a method of a class with a dynamic device, allowing the method to be\n",
    "    compiled with jax.jit for the specific device. This is needed since\n",
    "    @functools.partial(jax.jit, device=jax.devices(\"gpu\")[self._device_id]) is not\n",
    "    allowed in a class definition.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    method : callable\n",
    "        Class method to decorate. If DESC is running on GPU, the class should have\n",
    "        a device_id attribute.\n",
    "    \"\"\"\n",
    "\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        device = jax.devices(\"gpu\")[self._device_id]\n",
    "\n",
    "        # Compile the method with jax.jit for the specific device\n",
    "        wrapped = jax.jit(method, device=device)\n",
    "        return wrapped(self, *args, **kwargs)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Optimizable` and `Objective` classes\n",
    "\n",
    "These will be used during optimization problem. `Optimizable` stores the general info on the thing we try to optimize, and the `Objective` has the compute function that we use for the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.tree_util import register_pytree_node\n",
    "import copy\n",
    "\n",
    "\n",
    "class Optimizable:\n",
    "    def __init__(self, N, coefs):\n",
    "        self.N = N\n",
    "        self.coefs = coefs\n",
    "\n",
    "    def N(self):\n",
    "        return self.N\n",
    "\n",
    "    def coefs(self):\n",
    "        return self.coefs\n",
    "\n",
    "    def copy(self):\n",
    "        return copy.copy(self)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Optimizable(N={self.N}, coefs={self.coefs})\"\n",
    "\n",
    "\n",
    "def special_flatten_opt(obj):\n",
    "    \"\"\"Specifies a flattening recipe.\"\"\"\n",
    "    children = (obj.N, obj.coefs)\n",
    "    aux_data = None\n",
    "    return (children, aux_data)\n",
    "\n",
    "\n",
    "def special_unflatten_opt(aux_data, children):\n",
    "    \"\"\"Specifies an unflattening recipe.\"\"\"\n",
    "    obj = object.__new__(Optimizable)\n",
    "    obj.N = children[0]\n",
    "    obj.coefs = children[1]\n",
    "    return obj\n",
    "\n",
    "\n",
    "class Objective:\n",
    "    def __init__(self, opt, grid, target, device_id=0):\n",
    "        self.opt = opt\n",
    "        self.grid = grid\n",
    "        self.target = target\n",
    "        self.built = False\n",
    "        self._device_id = device_id\n",
    "\n",
    "    def build(self):\n",
    "        # the transform matrix A such that A @ coefs gives the\n",
    "        # values of the function at the grid points\n",
    "        self.A = jnp.vstack([jnp.cos(i * self.grid) for i in range(self.opt.N)]).T\n",
    "        self.built = True\n",
    "\n",
    "    @jit_with_device\n",
    "    def compute_error(self, coefs, A=None):\n",
    "        if A is None:\n",
    "            A = self.A\n",
    "        vals = A @ coefs\n",
    "        return vals - self.target\n",
    "\n",
    "    @jit_with_device\n",
    "    def jac_error(self, coefs, A=None):\n",
    "        if A is None:\n",
    "            A = self.A\n",
    "        return jax.jacfwd(self.compute_error)(coefs, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use this classes in Jax transformed functions, we have to register them as proper pytrees. This means we have to define flattening and unflattening functions that converts the object to a tuple and from that tuple back to the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_flatten_obj(obj):\n",
    "    \"\"\"Specifies a flattening recipe.\"\"\"\n",
    "    children = (obj.opt, obj.grid, obj.target, obj.A)\n",
    "    aux_data = (obj.built, obj._device_id)\n",
    "    return (children, aux_data)\n",
    "\n",
    "\n",
    "def special_unflatten_obj(aux_data, children):\n",
    "    \"\"\"Specifies an unflattening recipe.\"\"\"\n",
    "    obj = object.__new__(Objective)\n",
    "    obj.opt = children[0]\n",
    "    obj.grid = children[1]\n",
    "    obj.target = children[2]\n",
    "    obj.A = children[3]\n",
    "    obj.built = aux_data[0]\n",
    "    obj._device_id = aux_data[1]\n",
    "    return obj\n",
    "\n",
    "\n",
    "# Global registration\n",
    "register_pytree_node(Optimizable, special_flatten_opt, special_unflatten_opt)\n",
    "register_pytree_node(Objective, special_flatten_obj, special_unflatten_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Optimization\n",
    "We will use a very simple optimization problem in which we try to find the cosine fit to a function. `Optimizable` class has the max resolution and coefficients information. Coefficients are the things we change to get a proper fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.148645\n",
      "0.0009175726\n"
     ]
    }
   ],
   "source": [
    "N = 40\n",
    "num_nodes = 30\n",
    "coefs = np.zeros(N)\n",
    "coefs[2] = 3\n",
    "eq = Optimizable(N, coefs)\n",
    "grid = jnp.linspace(-jnp.pi, jnp.pi, num_nodes, endpoint=False)\n",
    "target = grid**2\n",
    "obj = Objective(eq, grid, target)\n",
    "obj.build()\n",
    "\n",
    "print(jnp.linalg.norm(obj.compute_error(eq.coefs, obj.A)))\n",
    "while jnp.linalg.norm(obj.compute_error(eq.coefs, obj.A)) > 1e-3:\n",
    "    eq.coefs = eq.coefs - 1e-1 * jnp.linalg.pinv(\n",
    "        obj.jac_error(eq.coefs, obj.A)\n",
    "    ) @ obj.compute_error(eq.coefs, obj.A)\n",
    "print(jnp.linalg.norm(obj.compute_error(eq.coefs, obj.A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define `ObjectiveFunction` for multiple `Objective` cases\n",
    "Above example had one `Objective`, but we might have multiple objective we try to minimize at the same time. For this, we will define `ObjectiveFunction` class that introduces the proper `compute` and `jac` definitions by concatenation of individual `Objective`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple GPU\n",
    "Our main goal by dividing the single `Objective` into multiple ones and then merge them by `ObjectiveFunction` is to execute the computation of each objective on different device to reduce the memory!\n",
    "\n",
    "Normal `jnp.concatenate` doesn't work if each array lives on different GPU. That is why we will create a new helper function to achieve this. Unfortunately, later in the optimization, we will use QR or SVD calculations which can only use 1 device, therefore, we will put the resultant array to GPU1 (if there is enough memory) or to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pconcat(arrays, mode=\"concat\"):\n",
    "    \"\"\"Concatenate arrays that live on different devices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arrays : list of jnp.ndarray\n",
    "        Arrays to concatenate.\n",
    "    mode : str\n",
    "        \"concat:, \"hstack\" or \"vstack. Default is \"concat\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out : jnp.ndarray\n",
    "        Concatenated array that lives on CPU.\n",
    "    \"\"\"\n",
    "    devices = nvgpu.gpu_info()\n",
    "    mem_avail = devices[0][\"mem_total\"] - devices[0][\"mem_used\"]\n",
    "    # we will use either CPU or GPU[0] for the matrix decompositions, so the\n",
    "    # array of float64 should fit into single device\n",
    "    size = jnp.array([x.size for x in arrays])\n",
    "    size = jnp.sum(size)\n",
    "    if size * 8 / (1024**3) > mem_avail:\n",
    "        device = jax.devices(\"cpu\")[0]\n",
    "    else:\n",
    "        device = jax.devices(\"gpu\")[0]\n",
    "\n",
    "    if mode == \"concat\":\n",
    "        out = jnp.concatenate([jax.device_put(x, device=device) for x in arrays])\n",
    "    elif mode == \"hstack\":\n",
    "        out = jnp.hstack([jax.device_put(x, device=device) for x in arrays])\n",
    "    elif mode == \"vstack\":\n",
    "        out = jnp.vstack([jax.device_put(x, device=device) for x in arrays])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectiveFunctionParallel:\n",
    "    def __init__(self, objectives):\n",
    "        self.objectives = objectives\n",
    "        self.num_device = len(objectives)\n",
    "        self.built = False\n",
    "        targets = [obj.target for obj in self.objectives]\n",
    "        self.target = pconcat(targets)\n",
    "\n",
    "    def build(self):\n",
    "        for obj in self.objectives:\n",
    "            if not obj.built:\n",
    "                obj.build()\n",
    "\n",
    "        self.A = [obj.A for obj in self.objectives]\n",
    "        self.built = True\n",
    "\n",
    "    def compute_error(self, coefs=None, A=None):\n",
    "        if A is None:\n",
    "            A = self.A\n",
    "        if coefs is None:\n",
    "            coefs = [obj.opt.coefs for obj in self.objectives]\n",
    "\n",
    "        # we need to move coefi to corresponding GPU because jit compiled\n",
    "        # functions only accept data from same device. Since, we try to use\n",
    "        # same obj.opt for all, and we store that on first GPU, hence coefs\n",
    "        # is a list of coefficients that are on first GPU. We could have\n",
    "        # Objectives without opt attribute but this a design choice we would\n",
    "        # like to keep\n",
    "        fs = [\n",
    "            obj.compute_error(\n",
    "                jax.device_put(coefi, device=jax.devices(\"gpu\")[obj._device_id]), Ai\n",
    "            )\n",
    "            for obj, coefi, Ai in zip(self.objectives, coefs, A)\n",
    "        ]\n",
    "        return pconcat(fs)\n",
    "\n",
    "    def jac_error(self, coefs=None, A=None):\n",
    "        if A is None:\n",
    "            A = self.A\n",
    "        if coefs is None:\n",
    "            coefs = [obj.opt.coefs for obj in self.objectives]\n",
    "        fs = [\n",
    "            obj.jac_error(\n",
    "                jax.device_put(coefi, device=jax.devices(\"gpu\")[obj._device_id]), Ai\n",
    "            )\n",
    "            for obj, coefi, Ai in zip(self.objectives, coefs, A)\n",
    "        ]\n",
    "        return pconcat(fs)\n",
    "\n",
    "    def _flatten(obj):\n",
    "        \"\"\"Specifies a flattening recipe.\"\"\"\n",
    "        children = (obj.objectives, obj.target, obj.A)\n",
    "        aux_data = (obj.built,)\n",
    "        return (children, aux_data)\n",
    "\n",
    "    @classmethod\n",
    "    def _unflatten(cls, aux_data, children):\n",
    "        \"\"\"Specifies an unflattening recipe.\"\"\"\n",
    "        cls.objectives = children[0]\n",
    "        cls.target = children[1]\n",
    "        cls.A = children[2]\n",
    "        cls.built = aux_data[0]\n",
    "        return cls\n",
    "\n",
    "\n",
    "register_pytree_node(\n",
    "    ObjectiveFunctionParallel,\n",
    "    ObjectiveFunctionParallel._flatten,\n",
    "    ObjectiveFunctionParallel._unflatten,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 40\n",
    "num_nodes = 15\n",
    "coefs = np.zeros(N)\n",
    "coefs[2] = 3\n",
    "eq = Optimizable(N, coefs)\n",
    "grid1 = jnp.linspace(-jnp.pi, 0, num_nodes, endpoint=False)\n",
    "grid2 = jnp.linspace(0, jnp.pi, num_nodes, endpoint=False)\n",
    "grid3 = jnp.concatenate([grid1, grid2])\n",
    "target1 = grid1**2\n",
    "target2 = grid2**2\n",
    "target3 = grid3**2\n",
    "\n",
    "obj1 = Objective(eq, grid1, target1, device_id=0)\n",
    "obj2 = Objective(eq, grid2, target2, device_id=1)\n",
    "obj1.build()\n",
    "obj2.build()\n",
    "\n",
    "# we will put different objectives to different devices\n",
    "obj1 = jax.device_put(obj1, jax.devices(\"gpu\")[0])\n",
    "obj2 = jax.device_put(obj2, jax.devices(\"gpu\")[1])\n",
    "# if we don't assign the eq again, there will be no connection\n",
    "# between obj.opt.coefs. Since they are supposed to be the same optimizable,\n",
    "# they need to have same pointers (jax.device_put creates a copy which has\n",
    "# different memory location)\n",
    "obj1.opt = eq\n",
    "obj2.opt = eq\n",
    "\n",
    "objp_fun = ObjectiveFunctionParallel([obj1, obj2])\n",
    "objp_fun.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.148645\n",
      "0.00091715116\n"
     ]
    }
   ],
   "source": [
    "objective = objp_fun\n",
    "print(jnp.linalg.norm(objective.compute_error()))\n",
    "step = 0\n",
    "while jnp.linalg.norm(objective.compute_error()) > 1e-3:\n",
    "    eq.coefs = (\n",
    "        eq.coefs\n",
    "        - 1e-1 * jnp.linalg.pinv(objective.jac_error()) @ objective.compute_error()\n",
    "    )\n",
    "    step += 1\n",
    "\n",
    "print(jnp.linalg.norm(objective.compute_error()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desc-env [~/.conda/envs/desc-env/]",
   "language": "python",
   "name": "conda_desc-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
