{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "import nvgpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to JIT the methods of a class on specific devices\n",
    "# for convenience, we define a decorator that does this for us, this will use\n",
    "# the device attribute of the class to determine the device to JIT on\n",
    "def jit_with_device(method):\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        # Compile the method with jax.jit for the specific device\n",
    "        wrapped = jax.jit(method, device=self._device)\n",
    "        return wrapped(self, *args, **kwargs)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.tree_util import register_pytree_node\n",
    "import copy\n",
    "\n",
    "\n",
    "# This will be registered as proper pytree\n",
    "class Optimizable:\n",
    "    def __init__(self, N, coefs):\n",
    "        self.N = N\n",
    "        self.coefs = coefs\n",
    "        # actual Optimizable class has more attributes, but we don't need them here\n",
    "\n",
    "\n",
    "# This will be registered as proper pytree\n",
    "class Objective:\n",
    "    def __init__(self, opt, grid, target, device_id=0):\n",
    "        self.opt = opt\n",
    "        self.grid = grid\n",
    "        self.target = target\n",
    "        self._device_id = device_id\n",
    "        self._device = jax.devices(\"gpu\")[device_id]\n",
    "\n",
    "    def build(self):\n",
    "        # the transform matrix A such that A @ coefs gives the\n",
    "        # values of the function at the grid points\n",
    "        self.A = jnp.vstack([jnp.cos(i * self.grid) for i in range(self.opt.N)]).T\n",
    "\n",
    "    @jit_with_device\n",
    "    def compute_error(self, coefs, A=None):\n",
    "        if A is None:\n",
    "            A = self.A\n",
    "        vals = A @ coefs\n",
    "        return vals - self.target\n",
    "\n",
    "    @jit_with_device\n",
    "    def jac_error(self, coefs, A=None):\n",
    "        if A is None:\n",
    "            A = self.A\n",
    "        return jax.jacfwd(self.compute_error)(coefs, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pconcat(arrays):\n",
    "    \"\"\"Concatenate arrays from multiple devices\"\"\"\n",
    "    # we will use either CPU or GPU[0] for the matrix decompositions, so the\n",
    "    # array of float64 should fit into single device\n",
    "    device = jax.devices(\"gpu\")[0]\n",
    "    out = jnp.concatenate([jax.device_put(x, device=device) for x in arrays])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be registered as proper pytree\n",
    "class ObjectiveFunctionParallel:\n",
    "    def __init__(self, objectives):\n",
    "        self.objectives = objectives\n",
    "        self.num_device = len(objectives)\n",
    "\n",
    "    def build(self):\n",
    "        # construct the constant arrays\n",
    "        ...\n",
    "\n",
    "    def compute_error(self, coefs=None, A=None):\n",
    "        # compute the error for each objective and concatenate them\n",
    "        fs = [\n",
    "            obj.compute_error(jax.device_put(coefi, device=obj._device), Ai)\n",
    "            for obj, coefi, Ai in zip(self.objectives, coefs, A)\n",
    "        ]\n",
    "        return pconcat(fs)\n",
    "\n",
    "    def jac_error(self, coefs=None, A=None):\n",
    "        # compute the jacobian for each objective and concatenate them\n",
    "        fs = [\n",
    "            obj.jac_error(jax.device_put(coefi, device=obj._device), Ai)\n",
    "            for obj, coefi, Ai in zip(self.objectives, coefs, A)\n",
    "        ]\n",
    "        return pconcat(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 40\n",
    "num_nodes = 15\n",
    "coefs = np.zeros(N)\n",
    "coefs[2] = 3\n",
    "eq = Optimizable(N, coefs)\n",
    "grid1 = jnp.linspace(-jnp.pi, 0, num_nodes, endpoint=False)\n",
    "grid2 = jnp.linspace(0, jnp.pi, num_nodes, endpoint=False)\n",
    "grid3 = jnp.concatenate([grid1, grid2])\n",
    "target1 = grid1**2\n",
    "target2 = grid2**2\n",
    "target3 = grid3**2\n",
    "\n",
    "obj1 = Objective(eq, grid1, target1, device_id=0)\n",
    "obj2 = Objective(eq, grid2, target2, device_id=1)\n",
    "obj1.build()\n",
    "obj2.build()\n",
    "\n",
    "# we will put different objectives to different devices\n",
    "obj1 = jax.device_put(obj1, jax.devices(\"gpu\")[0])\n",
    "obj2 = jax.device_put(obj2, jax.devices(\"gpu\")[1])\n",
    "# if we don't assign the eq again, there will be no connection\n",
    "# between obj.opt.coefs. Since they are supposed to be the same optimizable,\n",
    "# they need to have same pointers (jax.device_put creates a copy which has\n",
    "# different memory location)\n",
    "obj1.opt = eq\n",
    "obj2.opt = eq\n",
    "\n",
    "objp_fun = ObjectiveFunctionParallel([obj1, obj2])\n",
    "objp_fun.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.148645\n",
      "0.00091715116\n"
     ]
    }
   ],
   "source": [
    "objective = objp_fun\n",
    "print(jnp.linalg.norm(objective.compute_error()))\n",
    "step = 0\n",
    "while jnp.linalg.norm(objective.compute_error()) > 1e-3:\n",
    "    eq.coefs = (\n",
    "        eq.coefs\n",
    "        - 1e-1 * jnp.linalg.pinv(objective.jac_error()) @ objective.compute_error()\n",
    "    )\n",
    "    step += 1\n",
    "\n",
    "print(jnp.linalg.norm(objective.compute_error()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desc-env [~/.conda/envs/desc-env/]",
   "language": "python",
   "name": "conda_desc-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
