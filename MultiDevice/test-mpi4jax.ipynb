{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\".\"))\n",
    "sys.path.append(os.path.abspath(\"../../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(linewidth=np.inf, precision=4, suppress=True, threshold=sys.maxsize)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.graph_objects as go\n",
    "import functools\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from desc import set_device\n",
    "\n",
    "num_device = 2\n",
    "# _set_cpu_count(num_device)\n",
    "set_device(\"gpu\", num_device=num_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESC version 0.13.0+1543.g3edf125e0,using JAX backend, jax version=0.5.0, jaxlib version=0.5.0, dtype=float64\n",
      "Using 2 device:\n",
      "\t Device 0: NVIDIA A100-SXM4-40GB (id=0) with 40.00 GB available memory\n",
      "\t Device 1: NVIDIA A100-SXM4-40GB (id=1) with 40.00 GB available memory\n"
     ]
    }
   ],
   "source": [
    "import desc\n",
    "\n",
    "from desc.basis import *\n",
    "from desc.backend import *\n",
    "from desc.compute import *\n",
    "from desc.coils import *\n",
    "from desc.equilibrium import *\n",
    "from desc.examples import *\n",
    "from desc.grid import *\n",
    "from desc.geometry import *\n",
    "\n",
    "from desc.objectives import *\n",
    "from desc.objectives.objective_funs import *\n",
    "from desc.objectives.getters import *\n",
    "from desc.objectives.normalization import compute_scaling_factors\n",
    "from desc.objectives.utils import *\n",
    "from desc.optimize._constraint_wrappers import *\n",
    "\n",
    "from desc.transform import Transform\n",
    "from desc.plotting import *\n",
    "from desc.optimize import *\n",
    "from desc.perturbations import *\n",
    "from desc.profiles import *\n",
    "from desc.compat import *\n",
    "from desc.utils import *\n",
    "\n",
    "from desc.__main__ import main\n",
    "from desc.vmec_utils import vmec_boundary_subspace\n",
    "from desc.input_reader import InputReader\n",
    "from desc.continuation import solve_continuation_automatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mpi4py.MPI.Intracomm object at 0x14f333fff510> 0 1\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import mpi4jax.mpi4jax as mpi4jax\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "print(comm, rank, size)\n",
    "\n",
    "@jax.jit\n",
    "def foo(arr):\n",
    "   arr = arr + rank\n",
    "   arr_sum, _ = mpi4jax.allreduce(arr, op=MPI.SUM, comm=comm)\n",
    "   return arr_sum\n",
    "\n",
    "a = jnp.zeros((3, 3))\n",
    "result = foo(a)\n",
    "\n",
    "if rank == 0:\n",
    "   print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "There are not enough slots available in the system to satisfy the 1\n",
      "slots that were requested by the application:\n",
      "\n",
      "  /home/ye2698/.conda/envs/desc-mpi/bin/python\n",
      "\n",
      "Either request fewer slots for your application, or make more slots\n",
      "available for use.\n",
      "\n",
      "A \"slot\" is the Open MPI term for an allocatable unit where we can\n",
      "launch a process.  The number of slots available are defined by the\n",
      "environment in which Open MPI processes are run:\n",
      "\n",
      "  1. Hostfile, via \"slots=N\" clauses (N defaults to number of\n",
      "     processor cores if not provided)\n",
      "  2. The --host command line parameter, via a \":N\" suffix on the\n",
      "     hostname (N defaults to 1 if not provided)\n",
      "  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)\n",
      "  4. If none of a hostfile, the --host command line parameter, or an\n",
      "     RM is present, Open MPI defaults to the number of processor cores\n",
      "\n",
      "In all the above cases, if you want Open MPI to default to the number\n",
      "of hardware threads instead of the number of processor cores, use the\n",
      "--use-hwthread-cpus option.\n",
      "\n",
      "Alternatively, you can use the --oversubscribe option to ignore the\n",
      "number of available slots when deciding the number of processes to\n",
      "launch.\n",
      "--------------------------------------------------------------------------\n",
      "Exception in thread Thread-6 (_manager_spawn):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ye2698/.conda/envs/desc-mpi/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ye2698/.conda/envs/desc-mpi/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/ye2698/.conda/envs/desc-mpi/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ye2698/.conda/envs/desc-mpi/lib/python3.12/site-packages/mpi4py/futures/_core.py\", line 350, in _manager_spawn\n",
      "    comm = serialized(client_spawn)(pyexe, pyargs, nprocs, info)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ye2698/.conda/envs/desc-mpi/lib/python3.12/site-packages/mpi4py/futures/_core.py\", line 1058, in client_spawn\n",
      "    comm = MPI.COMM_SELF.Spawn(python_exe, args, max_workers, info)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/mpi4py/MPI.src/Comm.pyx\", line 2544, in mpi4py.MPI.Intracomm.Spawn\n",
      "mpi4py.MPI.Exception: MPI_ERR_SPAWN: could not spawn processes\n"
     ]
    }
   ],
   "source": [
    "from mpi4py.futures import MPIPoolExecutor\n",
    "from mpi4py import MPI\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import mpi4jax\n",
    "\n",
    "def mpi_worker(rank):\n",
    "    comm = MPI.COMM_WORLD\n",
    "    size = comm.Get_size()\n",
    "    print(comm, rank, size)\n",
    "\n",
    "    @jax.jit\n",
    "    def foo(arr):\n",
    "        arr = arr + rank\n",
    "        arr_sum, _ = mpi4jax.allreduce(arr, op=MPI.SUM, comm=comm)\n",
    "        return arr_sum\n",
    "\n",
    "    a = jnp.zeros((3, 3))\n",
    "    result = foo(a)\n",
    "    return result if rank == 0 else None\n",
    "\n",
    "def main():\n",
    "    with MPIPoolExecutor() as executor:\n",
    "        ranks = list(range(executor._max_workers))\n",
    "        results = list(executor.map(mpi_worker, ranks))\n",
    "        \n",
    "        for res in results:\n",
    "            if res is not None:\n",
    "                print(res)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desc-mpi [~/.conda/envs/desc-mpi/]",
   "language": "python",
   "name": "conda_desc-mpi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
